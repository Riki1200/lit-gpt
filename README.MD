# LitGPT

**LitGPT** is a lightweight chat application that runs entirely **locally on your device**, providing a private and efficient AI experience without needing an internet connection. It uses [**llama.cpp**](https://github.com/ggerganov/llama.cpp), a powerful C++ library that enables **offline inference of LLMs** (Large Language Models) optimized for local devices such as smartphones, laptops, or edge devices.

---

<img src="Simulator Screenshot - iPhone 16 Pro - 2025-04-13 at 20.34.15.png" alt="Screenshot" width="300"/>

<img src="Simulator Screenshot - iPhone 16 Pro - 2025-04-13 at 20.34.24.png" alt="Screenshot" width="300"/>


## 🚀 Key Features

- 💡 **Local Execution** – Runs completely on your device — no server, no cloud, no data sent online.
- ⚙️ **Powered by llama.cpp** – Utilizes optimized inference to run open-source models efficiently.
- 📱 **Mobile Friendly** – Designed to work on modern mobile phones without sacrificing performance.
- 🔒 **Privacy First** – All processing happens locally, so your conversations never leave your device.
- ⚡ **Lightweight & Fast** – Minimal resource usage while still providing impressive results.

---

## 🧠 How It Works

LitGPT integrates with `llama.cpp` to load quantized language models (such as LLaMA, Mistral, or TinyLLM variants). Once a model is loaded, it can generate responses to user prompts in real time — all **offline**.

---

## 📋 Requirements

- A device with sufficient RAM and CPU to run LLMs (most modern smartphones or desktops).
- A compatible quantized model supported by `llama.cpp`.
- Installation of `llama.cpp` and the LitGPT frontend (or mobile app).

---

## 💡 Use Cases

- Chat privately with an AI assistant.
- Take notes with natural language.
- Ask questions offline when traveling.
- Learn about AI development and edge machine learning.

---

## 🔗 Resources

- [llama.cpp GitHub](https://github.com/ggml-org/llama.cpp)
---

*LitGPT is ideal for users who value privacy, portability, and performance in an AI assistant.*
